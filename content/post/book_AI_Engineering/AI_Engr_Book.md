# AI Engr.

## Preface
- What This Book Is About
- What This Book Is Not
- Who This Book Is For
- Navigating This Book
- Conventions Used in This Book
- Using Code Examples
- Oâ€™Reilly Online Learning
- How to Contact Us
- Acknowledgments

## 1-3. Apps & Foundation Models
### Intro. to Building AI Applications
- The Rise of AI Engr.
    - From Language Models to Large Language Models
    - From Large Language Models to Foundation Models
    - From Foundation Models to AI Engr.
- Use Cases
    - Coding
    - Image & Video Production
    - Writing
    - Education
    - Conversational Bots
    - Info. Aggregation
    - Data Organization
    - Workflow Automation
- Planning
    - Use Case Eval.
    - Setting Expectations
    - Milestone Planning
    - Maintenance
- The AI Engr. Stack
    - Three Layers
    - AI Engr. v.s. ML Engr.
    - AI Engr. v.s. Full-Stack Engr.
### Understanding
- Training Data
    - Multilingual Models
    - Domain-Specific Models
- Modeling
    - Architecture
    - Size
- Post-Training
    - Supervised Finetuning
    - Preference Finetuning
- Sampling
    - Fundamentals
    - Strategies
    - Test Time Compute
    - Structured Outputs
    - Probabilistic Nature of AI
### Eval. Methodology
- Challenges of Evaluating
- Understanding Language Modeling Metrics
    - Entropy
    - Cross Entropy
    - Bits-per-Character & Bits-per-Byte
    - Perplexity
    - Perplexity Interpretation & Use Cases
- Exact Eval.
    - Functional Correctness
    - Similarity Measurements Against Reference Data
    - Intro. to Embedding
- AI as a Judge
    - Why AI as a Judge?
    - How to Use AI as a Judge
    - Limitations of AI as a Judge
    - What Models Can Act as Judges?
- Ranking Models with Comparative Eval.
    - Challenges
    - The Future

## 4. Eval. AI Systems
- Criteria
    - Domain-Specific Capability
    - Generation Capability
    - Instruction-Following Capability
    - Cost & Latency
- Model Selection
    - Workflow
    - Build v.s. Buy
    - Navigate Public Benchmarks
- Design Your Eval. Pipeline
    - Step 1. Evaluate All Components
    - Step 2. Create an Eval. Guideline
    - Step 3. Define Eval. Methods & Data

## 5. Prompt Engr.
- Intro. to Prompting
    - In-Context Learning: Zero-Shot & Few-Shot
    - System Prompt & User Prompt
    - Context Length & Context Efficiency
- Best Practices
    - Write Clear & Explicit Instructions
    - Provide Sufficient Context
    - Break Complex Tasks into Simpler Subtasks
    - Give the Model Time to Think
    - Iterate on Your Prompts
    - Evaluate Prompt Engr. Tools
    - Organize & Version Prompts
- Defensive
    - Proprietary Prompts & Reverse Prompt Engr.
    - Jailbreaking & Prompt Injection
    - Info. Extraction
    - Defenses Against AI Attacks

## 6. RAG & Agents
- RAG
    - Architecture
    - Retrieval Algorithms
    - Retrieval Opt.
    - Beyond Texts
- Agents
    - Tools
    - Planning
    - Failure Modes & Eval.
- Memory

## 7. Finetuning
- When to Finetune
    - Reasons to Finetune
    - Reasons Not to Finetune
    - Finetuning & RAG
- Memory Bottlenecks
    - Backpropagation & Trainable Parameters
    - Memory Math
    - Numerical Representations
    - Quantization
- Techniques
    - Parameter-Efficient Finetuning
    - Model Merging & Multi-Task Finetuning
    - Finetuning Tactics

## 8. Dataset Engr.
- Data Curation
    - Quality
    - Coverage
    - Quantity
    - Acquisition & Annotation
- Data Augmentation & Synthesis
    - Why Data Synthesis
    - Traditional Data Synthesis Techniques
    - AI-Powered Data Synthesis
    - Model Distillation
- Data Processing
    - Inspect Data
    - Deduplicate Data
    - Clean & Filter Data
    - Format Data

## 9. Inference Opt.
- Understanding
    - Inference Performance Metrics
    - AI Accelerators
- Opt.
    - Model Opt.
    - Inference Service Opt.

## 10. AI Engr. Architecture & User Feedback
- AI Engr. Architecture
- User Feedback